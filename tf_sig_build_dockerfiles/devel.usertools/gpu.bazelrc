# This bazelrc can build a GPU-supporting TF package.

# Convenient cache configurations
# Use a cache directory mounted to /tf/cache. Very useful!
build:sigbuild_local_cache --disk_cache=/tf/cache
# Use the public-access TF DevInfra cache (read only)
build:sigbuild_remote_cache --remote_cache="https://storage.googleapis.com/tensorflow-devinfra-bazel-cache" --remote_upload_local_results=false
# Write to the TF DevInfra cache (only works for internal TF CI)
build:sigbuild_remote_cache_push --remote_cache="https://storage.googleapis.com/tensorflow-devinfra-bazel-cache" --google_default_credentials

# Use Python 3.X as installed in container image
build --action_env PYTHON_BIN_PATH="/usr/bin/python3"
build --action_env PYTHON_LIB_PATH="/usr/lib/tf_python"
build --python_path="/usr/bin/python3"

# Build TensorFlow v2
build --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1

# Prevent double-compilation of some TF code, ref. b/183279666 (internal)
# > TF's gen_api_init_files has a genrule to run the core TensorFlow code
# > on the host machine. If we don't have --distinct_host_configuration=false,
# > the core TensorFlow code will be built once for the host and once for the
# > target platform.
# See also https://docs.bazel.build/versions/master/guide.html#build-configurations-and-cross-compilation
build --distinct_host_configuration=false

# Target the AVX instruction set
build --copt=-mavx --host_copt=-mavx

# Store performance profiling log in the mounted artifact directory.
# The profile can be viewed by visiting chrome://tracing in a Chrome browser.
# See https://docs.bazel.build/versions/main/skylark/performance.html#performance-profiling
build --profile=/tf/pkg/profile.json

# Store the execution log binary in the mounted artifact directory.
# This log is recommended to debug missing cache hit on the same machine and on different machines.
# See more at step 2 and 4 in:
# https://docs.bazel.build/versions/main/remote-caching-debug.html
build --execution_log_binary_file=/tf/pkg/exec_cuda.log

# CUDA: Set up compilation CUDA version and paths
build --@local_config_cuda//:enable_cuda
build --repo_env TF_NEED_CUDA=1
build --action_env=TF_CUDA_VERSION="11"
build --action_env=TF_CUDNN_VERSION="8"
build --action_env=CUDA_TOOLKIT_PATH="/usr/local/cuda-11.2"
build --action_env=GCC_HOST_COMPILER_PATH="/dt7/usr/bin/gcc"
build --action_env=LD_LIBRARY_PATH="/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/tensorrt/lib"
build --crosstool_top=@ubuntu18.04-gcc7_manylinux2010-cuda11.2-cudnn8.1-tensorrt7.2_config_cuda//crosstool:toolchain

# CUDA: Enable TensorRT optimizations
# https://developer.nvidia.com/tensorrt
build --repo_env TF_NEED_TENSORRT=1

# CUDA: Select supported compute capabilities (supported graphics cards).
# This is the same as the official TensorFlow builds.
# See https://developer.nvidia.com/cuda-gpus#compute
# TODO(angerson, perfinion): What does sm_ vs compute_ mean?
# TODO(angerson, perfinion): How can users select a good value for this?
build --repo_env=TF_CUDA_COMPUTE_CAPABILITIES="sm_35,sm_50,sm_60,sm_70,sm_75,compute_80"
